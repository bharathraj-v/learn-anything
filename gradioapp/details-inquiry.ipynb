{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A conversational agent that enquires user details and curates a json file from the information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Required format to be curated:\\\n",
    "{ \\\n",
    "  name: \"\",\\\n",
    "  age: 0,\\\n",
    "  goal: \"\",\\\n",
    "  educational background: \"\",\\\n",
    "  level/approach: \"\",\\\n",
    "  depth of the explanations: \"\",\\\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "details = {\n",
    "    \"name\": \"\",\n",
    "    \"age\": 0,\n",
    "    \"goal\": \"\",\n",
    "    \"educational_background\": \"\",\n",
    "    \"approach\": \"\",\n",
    "    \"topic_depth\": \"\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example conversation chain from langchain docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, what's up?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Hi there! I'm doing great. I'm currently helping a customer with a technical issue. How about you?\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.chains import LLMChain, ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory, ConversationSummaryMemory, ChatMessageHistory\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "conversation_with_summary = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory=ConversationSummaryMemory(llm=OpenAI()),\n",
    "    verbose=True\n",
    ")\n",
    "conversation_with_summary.predict(input=\"Hi, what's up?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "The human asks the AI how it is doing and the AI responds that it is helping a customer with a technical issue.\n",
      "Human: I'm doing great, I was just wondering if you could help me with my English.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Sure, I'd be happy to help you with your English. What kind of help do you need?\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_summary.predict(input=\"I'm doing great, I was just wondering if you could help me with my English.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "The human asks the AI how it is doing and the AI responds that it is helping a customer with a technical issue. The human then asks for help with their English, to which the AI responds positively and inquires what kind of help is needed.\n",
      "Human: Just explain to me what's the point of english? like in the technical issue you're solving, how is english helping you?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' English is helping me to communicate with the customer and understand their technical issue. It is also helping me to explain the technical issue to the customer in a way that they can understand.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_summary.predict(input=\"Just explain to me what's the point of english? like in the technical issue you're solving, how is english helping you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Converstation chain class works well but we need a custom chain for this use case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom conversational chain for details enquiry "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class ConversationChain in module langchain.chains.conversation.base:\n",
      "\n",
      "class ConversationChain(langchain.chains.llm.LLMChain)\n",
      " |  ConversationChain(*, memory: langchain.schema.memory.BaseMemory = None, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager] = None, verbose: bool = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, prompt: langchain.schema.prompt_template.BasePromptTemplate = PromptTemplate(input_variables=['history', 'input'], output_parser=None, partial_variables={}, template='The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\n\\nCurrent conversation:\\n{history}\\nHuman: {input}\\nAI:', template_format='f-string', validate_template=True), llm: langchain.schema.language_model.BaseLanguageModel, output_key: str = 'response', output_parser: langchain.schema.output_parser.BaseLLMOutputParser = None, return_final_only: bool = True, llm_kwargs: dict = None, input_key: str = 'input') -> None\n",
      " |  \n",
      " |  Chain to have a conversation and load context from memory.\n",
      " |  \n",
      " |  Example:\n",
      " |      .. code-block:: python\n",
      " |  \n",
      " |          from langchain import ConversationChain, OpenAI\n",
      " |  \n",
      " |          conversation = ConversationChain(llm=OpenAI())\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      ConversationChain\n",
      " |      langchain.chains.llm.LLMChain\n",
      " |      langchain.chains.base.Chain\n",
      " |      langchain.load.serializable.Serializable\n",
      " |      pydantic.main.BaseModel\n",
      " |      pydantic.utils.Representation\n",
      " |      langchain.schema.runnable.base.Runnable\n",
      " |      typing.Generic\n",
      " |      abc.ABC\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  validate_prompt_input_variables(values: Dict) -> Dict from pydantic.main.ModelMetaclass\n",
      " |      Validate that prompt input variables are consistent.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  input_keys\n",
      " |      Use this since so some prompt vars come from history.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  Config = <class 'langchain.chains.conversation.base.ConversationChain....\n",
      " |      Configuration for this pydantic object.\n",
      " |  \n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  __annotations__ = {'input_key': <class 'str'>, 'memory': <class 'langc...\n",
      " |  \n",
      " |  __class_vars__ = set()\n",
      " |  \n",
      " |  __config__ = <class 'langchain.chains.conversation.base.Config'>\n",
      " |  \n",
      " |  __custom_root_type__ = False\n",
      " |  \n",
      " |  __exclude_fields__ = {'callback_manager': True, 'callbacks': True}\n",
      " |  \n",
      " |  __fields__ = {'callback_manager': ModelField(name='callback_manager', ...\n",
      " |  \n",
      " |  __hash__ = None\n",
      " |  \n",
      " |  __include_fields__ = None\n",
      " |  \n",
      " |  __parameters__ = ()\n",
      " |  \n",
      " |  __post_root_validators__ = [(False, <function Chain.raise_callback_man...\n",
      " |  \n",
      " |  __pre_root_validators__ = []\n",
      " |  \n",
      " |  __private_attributes__ = {'_lc_kwargs': ModelPrivateAttr(default=Pydan...\n",
      " |  \n",
      " |  __schema_cache__ = {}\n",
      " |  \n",
      " |  __signature__ = <Signature (*, memory: langchain.schema.memory.B...: d...\n",
      " |  \n",
      " |  __validators__ = {'verbose': [<pydantic.class_validators.Validator obj...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain.chains.llm.LLMChain:\n",
      " |  \n",
      " |  async aapply(self, input_list: 'List[Dict[str, Any]]', callbacks: 'Callbacks' = None) -> 'List[Dict[str, str]]'\n",
      " |      Utilize the LLM generate method for speed gains.\n",
      " |  \n",
      " |  async aapply_and_parse(self, input_list: 'List[Dict[str, Any]]', callbacks: 'Callbacks' = None) -> 'Sequence[Union[str, List[str], Dict[str, str]]]'\n",
      " |      Call apply and then parse the results.\n",
      " |  \n",
      " |  async agenerate(self, input_list: 'List[Dict[str, Any]]', run_manager: 'Optional[AsyncCallbackManagerForChainRun]' = None) -> 'LLMResult'\n",
      " |      Generate LLM result from inputs.\n",
      " |  \n",
      " |  apply(self, input_list: 'List[Dict[str, Any]]', callbacks: 'Callbacks' = None) -> 'List[Dict[str, str]]'\n",
      " |      Utilize the LLM generate method for speed gains.\n",
      " |  \n",
      " |  apply_and_parse(self, input_list: 'List[Dict[str, Any]]', callbacks: 'Callbacks' = None) -> 'Sequence[Union[str, List[str], Dict[str, str]]]'\n",
      " |      Call apply and then parse the results.\n",
      " |  \n",
      " |  async apredict(self, callbacks: 'Callbacks' = None, **kwargs: 'Any') -> 'str'\n",
      " |      Format prompt with kwargs and pass to LLM.\n",
      " |      \n",
      " |      Args:\n",
      " |          callbacks: Callbacks to pass to LLMChain\n",
      " |          **kwargs: Keys to pass to prompt template.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Completion from LLM.\n",
      " |      \n",
      " |      Example:\n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              completion = llm.predict(adjective=\"funny\")\n",
      " |  \n",
      " |  async apredict_and_parse(self, callbacks: 'Callbacks' = None, **kwargs: 'Any') -> 'Union[str, List[str], Dict[str, str]]'\n",
      " |      Call apredict and then parse the results.\n",
      " |  \n",
      " |  async aprep_prompts(self, input_list: 'List[Dict[str, Any]]', run_manager: 'Optional[AsyncCallbackManagerForChainRun]' = None) -> 'Tuple[List[PromptValue], Optional[List[str]]]'\n",
      " |      Prepare prompts from inputs.\n",
      " |  \n",
      " |  create_outputs(self, llm_result: 'LLMResult') -> 'List[Dict[str, Any]]'\n",
      " |      Create outputs from response.\n",
      " |  \n",
      " |  generate(self, input_list: 'List[Dict[str, Any]]', run_manager: 'Optional[CallbackManagerForChainRun]' = None) -> 'LLMResult'\n",
      " |      Generate LLM result from inputs.\n",
      " |  \n",
      " |  predict(self, callbacks: 'Callbacks' = None, **kwargs: 'Any') -> 'str'\n",
      " |      Format prompt with kwargs and pass to LLM.\n",
      " |      \n",
      " |      Args:\n",
      " |          callbacks: Callbacks to pass to LLMChain\n",
      " |          **kwargs: Keys to pass to prompt template.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Completion from LLM.\n",
      " |      \n",
      " |      Example:\n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              completion = llm.predict(adjective=\"funny\")\n",
      " |  \n",
      " |  predict_and_parse(self, callbacks: 'Callbacks' = None, **kwargs: 'Any') -> 'Union[str, List[str], Dict[str, Any]]'\n",
      " |      Call predict and then parse the results.\n",
      " |  \n",
      " |  prep_prompts(self, input_list: 'List[Dict[str, Any]]', run_manager: 'Optional[CallbackManagerForChainRun]' = None) -> 'Tuple[List[PromptValue], Optional[List[str]]]'\n",
      " |      Prepare prompts from inputs.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from langchain.chains.llm.LLMChain:\n",
      " |  \n",
      " |  from_string(llm: 'BaseLanguageModel', template: 'str') -> 'LLMChain' from pydantic.main.ModelMetaclass\n",
      " |      Create LLMChain from LLM and template.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from langchain.chains.llm.LLMChain:\n",
      " |  \n",
      " |  lc_serializable\n",
      " |      Return whether or not the class is serializable.\n",
      " |  \n",
      " |  output_keys\n",
      " |      Will always return text key.\n",
      " |      \n",
      " |      :meta private:\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain.chains.base.Chain:\n",
      " |  \n",
      " |  __call__(self, inputs: Union[Dict[str, Any], Any], return_only_outputs: bool = False, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, *, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, run_name: Optional[str] = None, include_run_info: bool = False) -> Dict[str, Any]\n",
      " |      Execute the chain.\n",
      " |      \n",
      " |      Args:\n",
      " |          inputs: Dictionary of inputs, or single input if chain expects\n",
      " |              only one param. Should contain all inputs specified in\n",
      " |              `Chain.input_keys` except for inputs that will be set by the chain's\n",
      " |              memory.\n",
      " |          return_only_outputs: Whether to return only outputs in the\n",
      " |              response. If True, only new keys generated by this chain will be\n",
      " |              returned. If False, both input keys and new keys generated by this\n",
      " |              chain will be returned. Defaults to False.\n",
      " |          callbacks: Callbacks to use for this chain run. These will be called in\n",
      " |              addition to callbacks passed to the chain during construction, but only\n",
      " |              these runtime callbacks will propagate to calls to other objects.\n",
      " |          tags: List of string tags to pass to all callbacks. These will be passed in\n",
      " |              addition to tags passed to the chain during construction, but only\n",
      " |              these runtime tags will propagate to calls to other objects.\n",
      " |          metadata: Optional metadata associated with the chain. Defaults to None\n",
      " |          include_run_info: Whether to include run info in the response. Defaults\n",
      " |              to False.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A dict of named outputs. Should contain all outputs specified in\n",
      " |              `Chain.output_keys`.\n",
      " |  \n",
      " |  async acall(self, inputs: Union[Dict[str, Any], Any], return_only_outputs: bool = False, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, *, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, run_name: Optional[str] = None, include_run_info: bool = False) -> Dict[str, Any]\n",
      " |      Asynchronously execute the chain.\n",
      " |      \n",
      " |      Args:\n",
      " |          inputs: Dictionary of inputs, or single input if chain expects\n",
      " |              only one param. Should contain all inputs specified in\n",
      " |              `Chain.input_keys` except for inputs that will be set by the chain's\n",
      " |              memory.\n",
      " |          return_only_outputs: Whether to return only outputs in the\n",
      " |              response. If True, only new keys generated by this chain will be\n",
      " |              returned. If False, both input keys and new keys generated by this\n",
      " |              chain will be returned. Defaults to False.\n",
      " |          callbacks: Callbacks to use for this chain run. These will be called in\n",
      " |              addition to callbacks passed to the chain during construction, but only\n",
      " |              these runtime callbacks will propagate to calls to other objects.\n",
      " |          tags: List of string tags to pass to all callbacks. These will be passed in\n",
      " |              addition to tags passed to the chain during construction, but only\n",
      " |              these runtime tags will propagate to calls to other objects.\n",
      " |          metadata: Optional metadata associated with the chain. Defaults to None\n",
      " |          include_run_info: Whether to include run info in the response. Defaults\n",
      " |              to False.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A dict of named outputs. Should contain all outputs specified in\n",
      " |              `Chain.output_keys`.\n",
      " |  \n",
      " |  async ainvoke(self, input: Dict[str, Any], config: Optional[langchain.schema.runnable.config.RunnableConfig] = None, **kwargs: Any) -> Dict[str, Any]\n",
      " |      Default implementation of ainvoke, which calls invoke in a thread pool.\n",
      " |      Subclasses should override this method if they can run asynchronously.\n",
      " |  \n",
      " |  async arun(self, *args: Any, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, **kwargs: Any) -> Any\n",
      " |      Convenience method for executing chain.\n",
      " |      \n",
      " |      The main difference between this method and `Chain.__call__` is that this\n",
      " |      method expects inputs to be passed directly in as positional arguments or\n",
      " |      keyword arguments, whereas `Chain.__call__` expects a single input dictionary\n",
      " |      with all the inputs\n",
      " |      \n",
      " |      \n",
      " |      Args:\n",
      " |          *args: If the chain expects a single input, it can be passed in as the\n",
      " |              sole positional argument.\n",
      " |          callbacks: Callbacks to use for this chain run. These will be called in\n",
      " |              addition to callbacks passed to the chain during construction, but only\n",
      " |              these runtime callbacks will propagate to calls to other objects.\n",
      " |          tags: List of string tags to pass to all callbacks. These will be passed in\n",
      " |              addition to tags passed to the chain during construction, but only\n",
      " |              these runtime tags will propagate to calls to other objects.\n",
      " |          **kwargs: If the chain expects multiple inputs, they can be passed in\n",
      " |              directly as keyword arguments.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The chain output.\n",
      " |      \n",
      " |      Example:\n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              # Suppose we have a single-input chain that takes a 'question' string:\n",
      " |              await chain.arun(\"What's the temperature in Boise, Idaho?\")\n",
      " |              # -> \"The temperature in Boise is...\"\n",
      " |      \n",
      " |              # Suppose we have a multi-input chain that takes a 'question' string\n",
      " |              # and 'context' string:\n",
      " |              question = \"What's the temperature in Boise, Idaho?\"\n",
      " |              context = \"Weather report for Boise, Idaho on 07/03/23...\"\n",
      " |              await chain.arun(question=question, context=context)\n",
      " |              # -> \"The temperature in Boise is...\"\n",
      " |  \n",
      " |  dict(self, **kwargs: Any) -> Dict\n",
      " |      Dictionary representation of chain.\n",
      " |      \n",
      " |      Expects `Chain._chain_type` property to be implemented and for memory to be\n",
      " |          null.\n",
      " |      \n",
      " |      Args:\n",
      " |          **kwargs: Keyword arguments passed to default `pydantic.BaseModel.dict`\n",
      " |              method.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A dictionary representation of the chain.\n",
      " |      \n",
      " |      Example:\n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              chain.dict(exclude_unset=True)\n",
      " |              # -> {\"_type\": \"foo\", \"verbose\": False, ...}\n",
      " |  \n",
      " |  invoke(self, input: Dict[str, Any], config: Optional[langchain.schema.runnable.config.RunnableConfig] = None, **kwargs: Any) -> Dict[str, Any]\n",
      " |  \n",
      " |  prep_inputs(self, inputs: Union[Dict[str, Any], Any]) -> Dict[str, str]\n",
      " |      Validate and prepare chain inputs, including adding inputs from memory.\n",
      " |      \n",
      " |      Args:\n",
      " |          inputs: Dictionary of raw inputs, or single input if chain expects\n",
      " |              only one param. Should contain all inputs specified in\n",
      " |              `Chain.input_keys` except for inputs that will be set by the chain's\n",
      " |              memory.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A dictionary of all inputs, including those added by the chain's memory.\n",
      " |  \n",
      " |  prep_outputs(self, inputs: Dict[str, str], outputs: Dict[str, str], return_only_outputs: bool = False) -> Dict[str, str]\n",
      " |      Validate and prepare chain outputs, and save info about this run to memory.\n",
      " |      \n",
      " |      Args:\n",
      " |          inputs: Dictionary of chain inputs, including any inputs added by chain\n",
      " |              memory.\n",
      " |          outputs: Dictionary of initial chain outputs.\n",
      " |          return_only_outputs: Whether to only return the chain outputs. If False,\n",
      " |              inputs are also added to the final outputs.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A dict of the final chain outputs.\n",
      " |  \n",
      " |  run(self, *args: Any, callbacks: Union[List[langchain.callbacks.base.BaseCallbackHandler], langchain.callbacks.base.BaseCallbackManager, NoneType] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, **kwargs: Any) -> Any\n",
      " |      Convenience method for executing chain.\n",
      " |      \n",
      " |      The main difference between this method and `Chain.__call__` is that this\n",
      " |      method expects inputs to be passed directly in as positional arguments or\n",
      " |      keyword arguments, whereas `Chain.__call__` expects a single input dictionary\n",
      " |      with all the inputs\n",
      " |      \n",
      " |      Args:\n",
      " |          *args: If the chain expects a single input, it can be passed in as the\n",
      " |              sole positional argument.\n",
      " |          callbacks: Callbacks to use for this chain run. These will be called in\n",
      " |              addition to callbacks passed to the chain during construction, but only\n",
      " |              these runtime callbacks will propagate to calls to other objects.\n",
      " |          tags: List of string tags to pass to all callbacks. These will be passed in\n",
      " |              addition to tags passed to the chain during construction, but only\n",
      " |              these runtime tags will propagate to calls to other objects.\n",
      " |          **kwargs: If the chain expects multiple inputs, they can be passed in\n",
      " |              directly as keyword arguments.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The chain output.\n",
      " |      \n",
      " |      Example:\n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              # Suppose we have a single-input chain that takes a 'question' string:\n",
      " |              chain.run(\"What's the temperature in Boise, Idaho?\")\n",
      " |              # -> \"The temperature in Boise is...\"\n",
      " |      \n",
      " |              # Suppose we have a multi-input chain that takes a 'question' string\n",
      " |              # and 'context' string:\n",
      " |              question = \"What's the temperature in Boise, Idaho?\"\n",
      " |              context = \"Weather report for Boise, Idaho on 07/03/23...\"\n",
      " |              chain.run(question=question, context=context)\n",
      " |              # -> \"The temperature in Boise is...\"\n",
      " |  \n",
      " |  save(self, file_path: Union[pathlib.Path, str]) -> None\n",
      " |      Save the chain.\n",
      " |      \n",
      " |      Expects `Chain._chain_type` property to be implemented and for memory to be\n",
      " |          null.\n",
      " |      \n",
      " |      Args:\n",
      " |          file_path: Path to file to save the chain to.\n",
      " |      \n",
      " |      Example:\n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              chain.save(file_path=\"path/chain.yaml\")\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from langchain.chains.base.Chain:\n",
      " |  \n",
      " |  raise_callback_manager_deprecation(values: Dict) -> Dict from pydantic.main.ModelMetaclass\n",
      " |      Raise deprecation warning if callback_manager is used.\n",
      " |  \n",
      " |  set_verbose(verbose: Optional[bool]) -> bool from pydantic.main.ModelMetaclass\n",
      " |      Set the chain verbosity.\n",
      " |      \n",
      " |      Defaults to the global setting if not specified by the user.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from langchain.chains.base.Chain:\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from langchain.chains.base.Chain:\n",
      " |  \n",
      " |  __orig_bases__ = (<class 'langchain.load.serializable.Serializable'>, ...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain.load.serializable.Serializable:\n",
      " |  \n",
      " |  __init__(self, **kwargs: Any) -> None\n",
      " |      Create a new model by parsing and validating input data from keyword arguments.\n",
      " |      \n",
      " |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      " |  \n",
      " |  to_json(self) -> Union[langchain.load.serializable.SerializedConstructor, langchain.load.serializable.SerializedNotImplemented]\n",
      " |  \n",
      " |  to_json_not_implemented(self) -> langchain.load.serializable.SerializedNotImplemented\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from langchain.load.serializable.Serializable:\n",
      " |  \n",
      " |  lc_attributes\n",
      " |      Return a list of attribute names that should be included in the\n",
      " |      serialized kwargs. These attributes must be accepted by the\n",
      " |      constructor.\n",
      " |  \n",
      " |  lc_namespace\n",
      " |      Return the namespace of the langchain object.\n",
      " |      eg. [\"langchain\", \"llms\", \"openai\"]\n",
      " |  \n",
      " |  lc_secrets\n",
      " |      Return a map of constructor argument names to secret ids.\n",
      " |      eg. {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pydantic.main.BaseModel:\n",
      " |  \n",
      " |  __eq__(self, other: Any) -> bool\n",
      " |  \n",
      " |  __getstate__(self) -> 'DictAny'\n",
      " |  \n",
      " |  __iter__(self) -> 'TupleGenerator'\n",
      " |      so `dict(model)` works\n",
      " |  \n",
      " |  __repr_args__(self) -> 'ReprArgs'\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |  \n",
      " |  __setstate__(self, state: 'DictAny') -> None\n",
      " |  \n",
      " |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      " |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      " |      \n",
      " |      :param include: fields to include in new model\n",
      " |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      " |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      " |          the new model: you should trust this data\n",
      " |      :param deep: set to `True` to make a deep copy of the model\n",
      " |      :return: new model instance\n",
      " |  \n",
      " |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
      " |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      " |      \n",
      " |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pydantic.main.BaseModel:\n",
      " |  \n",
      " |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      " |      Same as update_forward_refs but will not raise exception\n",
      " |      when forward references are not defined.\n",
      " |  \n",
      " |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      " |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      " |      Default values are respected, but no other validation is performed.\n",
      " |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      " |  \n",
      " |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      " |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      " |  \n",
      " |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __fields_set__\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pydantic.utils.Representation:\n",
      " |  \n",
      " |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      " |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      " |  \n",
      " |  __repr__(self) -> 'unicode'\n",
      " |  \n",
      " |  __repr_name__(self) -> 'unicode'\n",
      " |      Name of the instance's class, used in __repr__.\n",
      " |  \n",
      " |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
      " |  \n",
      " |  __rich_repr__(self) -> 'RichReprResult'\n",
      " |      Get fields for Rich library\n",
      " |  \n",
      " |  __str__(self) -> 'unicode'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain.schema.runnable.base.Runnable:\n",
      " |  \n",
      " |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSequence[Input, Other]'\n",
      " |      Return self|value.\n",
      " |  \n",
      " |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Any], Other], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSequence[Other, Output]'\n",
      " |      Return value|self.\n",
      " |  \n",
      " |  async abatch(self, inputs: 'List[Input]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Optional[Any]') -> 'List[Output]'\n",
      " |      Default implementation of abatch, which calls ainvoke N times.\n",
      " |      Subclasses should override this method if they can batch more efficiently.\n",
      " |  \n",
      " |  async astream(self, input: 'Input', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      " |      Default implementation of astream, which calls ainvoke.\n",
      " |      Subclasses should override this method if they support streaming output.\n",
      " |  \n",
      " |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      " |      Default implementation of atransform, which buffers input and calls astream.\n",
      " |      Subclasses should override this method if they can start producing output while\n",
      " |      input is still being generated.\n",
      " |  \n",
      " |  batch(self, inputs: 'List[Input]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Optional[Any]') -> 'List[Output]'\n",
      " |      Default implementation of batch, which calls invoke N times.\n",
      " |      Subclasses should override this method if they can batch more efficiently.\n",
      " |  \n",
      " |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      " |      Bind arguments to a Runnable, returning a new Runnable.\n",
      " |  \n",
      " |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
      " |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      " |      by calling invoke() with each input.\n",
      " |  \n",
      " |  stream(self, input: 'Input', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      " |      Default implementation of stream, which calls invoke.\n",
      " |      Subclasses should override this method if they support streaming output.\n",
      " |  \n",
      " |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      " |      Default implementation of transform, which buffers input and then calls stream.\n",
      " |      Subclasses should override this method if they can start producing output while\n",
      " |      input is still being generated.\n",
      " |  \n",
      " |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      " |      Bind config to a Runnable, returning a new Runnable.\n",
      " |  \n",
      " |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException]]' = (<class 'Exception'>,)) -> 'RunnableWithFallbacks[Input, Output]'\n",
      " |  \n",
      " |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException]]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from typing.Generic:\n",
      " |  \n",
      " |  __class_getitem__(params) from pydantic.main.ModelMetaclass\n",
      " |      Parameterizes a generic class.\n",
      " |      \n",
      " |      At least, parameterizing a generic class is the *main* thing this method\n",
      " |      does. For example, for some generic class `Foo`, this is called when we\n",
      " |      do `Foo[int]` - there, with `cls=Foo` and `params=int`.\n",
      " |      \n",
      " |      However, note that this method is also called when defining generic\n",
      " |      classes in the first place with `class Foo(Generic[T]): ...`.\n",
      " |  \n",
      " |  __init_subclass__(*args, **kwargs) from pydantic.main.ModelMetaclass\n",
      " |      This method is called when a class is subclassed.\n",
      " |      \n",
      " |      The default implementation does nothing. It may be\n",
      " |      overridden to extend subclasses.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(ConversationChain)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'prompt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PromptTemplate\n\u001b[0;32m----> 3\u001b[0m custom_bot \u001b[38;5;241m=\u001b[39m ConversationChain(\n\u001b[1;32m      4\u001b[0m     llm\u001b[38;5;241m=\u001b[39mOpenAI(),\n\u001b[1;32m      5\u001b[0m     memory\u001b[38;5;241m=\u001b[39mConversationSummaryMemory(llm\u001b[38;5;241m=\u001b[39mOpenAI()),\n\u001b[1;32m      6\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      7\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.11/site-packages/langchain/load/serializable.py:74\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 74\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     75\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lc_kwargs \u001b[39m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.11/site-packages/pydantic/main.py:339\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.11/site-packages/pydantic/main.py:1102\u001b[0m, in \u001b[0;36mpydantic.main.validate_model\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.11/site-packages/langchain/chains/conversation/base.py:51\u001b[0m, in \u001b[0;36mConversationChain.validate_prompt_input_variables\u001b[0;34m(cls, values)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39mif\u001b[39;00m input_key \u001b[39min\u001b[39;00m memory_keys:\n\u001b[1;32m     47\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     48\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe input key \u001b[39m\u001b[39m{\u001b[39;00minput_key\u001b[39m}\u001b[39;00m\u001b[39m was also found in the memory keys \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     49\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00mmemory_keys\u001b[39m}\u001b[39;00m\u001b[39m) - please provide keys that don\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt overlap.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     50\u001b[0m     )\n\u001b[0;32m---> 51\u001b[0m prompt_variables \u001b[39m=\u001b[39m values[\u001b[39m\"\u001b[39m\u001b[39mprompt\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39minput_variables\n\u001b[1;32m     52\u001b[0m expected_keys \u001b[39m=\u001b[39m memory_keys \u001b[39m+\u001b[39m [input_key]\n\u001b[1;32m     53\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mset\u001b[39m(expected_keys) \u001b[39m!=\u001b[39m \u001b[39mset\u001b[39m(prompt_variables):\n",
      "\u001b[0;31mKeyError\u001b[0m: 'prompt'"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "custom_bot = ConversationChain(\n",
    "    llm=OpenAI(),\n",
    "    memory=ConversationSummaryMemory(llm=OpenAI()),\n",
    "    verbose=True,\n",
    "    prompt = \"\"\n",
    ")  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
